{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b71702eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatypus/Documents/stanford/cs109/challenge/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 images for train, 2000 images for test, 10000 total!\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "import csv\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "class Sample(TypedDict):\n",
    "    path: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "    image: np.ndarray\n",
    "    target: np.ndarray\n",
    "\n",
    "\n",
    "_coordinates_dict = {}\n",
    "\n",
    "def coordinates_from_path(filepath):\n",
    "    if filepath in _coordinates_dict:\n",
    "        return _coordinates_dict[filepath]  # (lat, lon)\n",
    "    return None\n",
    "\n",
    "def csv_loader(path):\n",
    "    coords = []\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        lat, lng = line.strip().split(\",\")\n",
    "        coords.append([float(lat), float(lng)])\n",
    "    return coords\n",
    "\n",
    "def load_images(path, output_csv=\"coordinates.csv\"):\n",
    "    global _coordinates_dict\n",
    "    items = []\n",
    "    _coordinates_dict = {}\n",
    "    \n",
    "    # Load coordinates from dataset\n",
    "    coords = csv_loader(f\"{path}/coords.csv\")\n",
    "    \n",
    "    # Load all png images\n",
    "    for image in os.listdir(path):\n",
    "        if \"png\" not in image:\n",
    "            continue\n",
    "        filepath = f\"{path}/{image}\"\n",
    "        items.append(filepath)\n",
    "        # Image index corresponds to coords index (0.png -> coords[0], etc.)\n",
    "        index = int(image.replace(\".png\", \"\"))\n",
    "        lat, lon = coords[index]\n",
    "        _coordinates_dict[filepath] = (lat, lon)\n",
    "    \n",
    "    # Output to local coordinates.csv\n",
    "    with open(output_csv, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"lat\", \"lon\"])\n",
    "        for filepath in items:\n",
    "            lat, lon = _coordinates_dict[filepath]\n",
    "            writer.writerow([lat, lon])\n",
    "\n",
    "    random.shuffle(items)\n",
    "    return items\n",
    "\n",
    "def train_test_split(paths, split=0.75, max=None):\n",
    "    paths = paths[:max] if max is not None else paths\n",
    "    split_index = int(len(paths) * split)\n",
    "    train, test = paths[:split_index], paths[split_index:]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# 10,000 images, zero-indexed.png\n",
    "path = kagglehub.dataset_download(\"paulchambaz/google-street-view\")\n",
    "path = f\"{path}/dataset\"\n",
    "images = load_images(path)\n",
    "train, test = train_test_split(images, split=0.8)\n",
    "\n",
    "print(f\"{len(train)} images for train, {len(test)} images for test, {len(images)} total!\")\n",
    "\n",
    "# The coordinates for the output-frame (how high resolution should the probability distribution be?)\n",
    "OUT_H, OUT_W = 32, 64\n",
    "EPSILON = 1e-12 # value for ensuring model does not hit 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eef4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "# I call the inputs (incoming nodes) \"x\", and outgoing (nodes that I affect) \"o\"\n",
    "\n",
    "class Layer():\n",
    "    compiled = False\n",
    "    len = None\n",
    "    x = None\n",
    "    o = None\n",
    "    mode = \"train\"\n",
    "    \n",
    "    def compile(self, inputs: int = None):\n",
    "        if self.len is None:\n",
    "            if inputs is None:\n",
    "                raise Exception(\"The layer has an undefined size\")\n",
    "            self.len = inputs\n",
    "        self.compiled = True\n",
    "    \n",
    "    def loss(self, lr):\n",
    "        pass\n",
    "\n",
    "    def set_mode(self, mode: Literal[\"train\", \"test\"]):\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "# ReLU and LeakyReLU together\n",
    "class ReLU(Layer):\n",
    "    def __init__(self, c=0):\n",
    "        self.c = c\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.o = np.where(x > 0, x, self.c * x)\n",
    "        return self.o\n",
    "    \n",
    "    def backward(self, dl):\n",
    "        return dl * np.where(self.x > 0, 1, self.c)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ReLU layer, with c={self.c}\"\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.o = 1 / (1 + np.exp(-x))\n",
    "        return self.o\n",
    "    \n",
    "    def backward(self, dl):\n",
    "        return dl * self.o * (1 - self.o)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Sigmoid layer\"\n",
    "\n",
    "\n",
    "class BatchNorm(Layer):\n",
    "    def __init__(self, momentum=0.9, epsilon=1e-5):\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.running_mean = None\n",
    "        self.running_var = None\n",
    "        \n",
    "    def compile(self, inputs):\n",
    "        self.len = inputs\n",
    "        self.running_mean = np.zeros(inputs)\n",
    "        self.running_var = np.ones(inputs)\n",
    "        self.gamma = np.ones(inputs)\n",
    "        self.beta = np.zeros(inputs)\n",
    "        self.compiled = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        if self.mode == \"train\":\n",
    "            self.mean = np.mean(x)\n",
    "            self.var = np.var(x) + self.epsilon\n",
    "            self.x_norm = (x - self.mean) / np.sqrt(self.var)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "        else:\n",
    "            self.x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        self.o = self.gamma * self.x_norm + self.beta\n",
    "        return self.o\n",
    "    \n",
    "    def backward(self, dl):\n",
    "        dx_norm = dl * self.gamma\n",
    "        return dx_norm / np.sqrt(self.var)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"BatchNorm layer\"\n",
    "\n",
    "\n",
    "class PatchLinear(Layer):\n",
    "    def __init__(self, outputs, wpn, decay):\n",
    "        self.len = outputs\n",
    "        self.wpn = wpn\n",
    "        self.decay = decay\n",
    "\n",
    "    def compile(self, _=None):\n",
    "        if _ is not None:\n",
    "            raise Exception(\"This layer must be the first layer\")\n",
    "            \n",
    "        self.weights = np.random.randn(self.len, self.wpn) * (1 / np.sqrt(self.wpn)) # N x Inputs\n",
    "        self.bias = np.zeros(self.len)  # N x 1\n",
    "        self.compiled = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.o = []\n",
    "        for patch, weights in zip(x, self.weights):\n",
    "            self.o.append(patch.T @ weights)  # dot product, returns float\n",
    "        return np.array(self.o) + self.bias\n",
    "\n",
    "    def backward(self, dl):\n",
    "        self.dW = np.zeros_like(self.weights)\n",
    "        self.db = np.zeros_like(self.bias)\n",
    "        dx = np.zeros_like(self.x) \n",
    "\n",
    "        for i, (patch, w) in enumerate(zip(self.x, self.weights)):\n",
    "            self.dW[i] = dl[i] * patch\n",
    "            self.db[i] = dl[i]\n",
    "            dx[i] = dl[i] * w\n",
    "        return dx\n",
    "\n",
    "    def loss(self, lr):\n",
    "        self.bias -= lr * self.db\n",
    "        self.weights -= lr * (self.dW + self.decay * self.weights)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Patch Linear layer, with {self.wpn} weights per neuron and {self.len} outputs\"\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, outputs, decay):\n",
    "        self.len = outputs\n",
    "        self.decay = decay\n",
    "\n",
    "    def compile(self, inputs):\n",
    "        if inputs is None and self.inputs is None:\n",
    "            raise Exception(\"This layer has an undefined size\")\n",
    "        self.inputs = inputs\n",
    "        self.weights = np.random.randn(self.len, inputs) / np.sqrt(inputs) # N x Inputs\n",
    "        self.bias = np.zeros(self.len) # N x 1\n",
    "        self.compiled = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.o = self.weights @ x + self.bias\n",
    "        return self.o\n",
    "            \n",
    "    def backward(self, dl):\n",
    "        self.dW = np.outer(dl, self.x) # (N) x (Inputs)\n",
    "        self.db = dl # x 1\n",
    "        return self.weights.T @ dl\n",
    "    \n",
    "    def loss(self, lr):\n",
    "        self.bias -= lr * self.db\n",
    "        self.weights -= lr * (self.dW + self.decay * self.weights)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Dense layer, with {self.inputs} inputs and {self.len} outputs\"\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        x_shift = x - np.max(x)\n",
    "        exp = np.exp(x_shift)\n",
    "        self.o = exp / np.sum(exp)\n",
    "        return self.o\n",
    "\n",
    "    def backward(self, dl):\n",
    "        return self.o * (dl - np.dot(dl, self.o))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Softmax layer\"\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == \"test\":\n",
    "            return x\n",
    "        self.mask = np.random.binomial(n=1, p=1-self.p, size=x.shape)\n",
    "        self.mask = self.mask * (1 / (1 - self.p))\n",
    "        self.o = x * self.mask\n",
    "        return self.o\n",
    "\n",
    "    def backward(self, dl):\n",
    "        return dl * self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Dropout layer, with p={self.p}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71443195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, decay=0.0):\n",
    "        self.layers: List[Layer] = []\n",
    "        self.decay = decay\n",
    "\n",
    "    def sigmoid(self):\n",
    "        self.layers.append(Sigmoid())\n",
    "        return self\n",
    "    \n",
    "    def relu(self):\n",
    "        self.layers.append(ReLU())\n",
    "        return self\n",
    "\n",
    "    def leaky_relu(self, c: float):\n",
    "        self.layers.append(ReLU(c=c))\n",
    "        return self\n",
    "    \n",
    "    def batchnorm(self):\n",
    "        self.layers.append(BatchNorm())\n",
    "        return self\n",
    "    \n",
    "    def patch_linear(self, outputs, wpn):\n",
    "        self.layers.append(PatchLinear(outputs, wpn, self.decay))\n",
    "        return self\n",
    "\n",
    "    def dense(self, outputs):\n",
    "        self.layers.append(Dense(outputs, self.decay))\n",
    "        return self\n",
    "\n",
    "    def softmax(self):\n",
    "        self.layers.append(Softmax())\n",
    "        return self\n",
    "    \n",
    "    def dropout(self, p):\n",
    "        self.layers.append(Dropout(p))\n",
    "        return self\n",
    "    \n",
    "    def compile(self):\n",
    "        inputs = None\n",
    "        for layer in self.layers:\n",
    "            layer.compile(inputs)\n",
    "            inputs = len(layer)\n",
    "\n",
    "    def set_mode(self, mode: Literal[\"train\", \"test\"]):\n",
    "        for layer in self.layers:\n",
    "            layer.set_mode(mode)\n",
    "\n",
    "    def evaluate(self, test_gen, test_count, alpha=0.5):\n",
    "        self.set_mode(\"test\")\n",
    "        loss_list = []\n",
    "        \n",
    "        for sample in tqdm(test_gen(), total=test_count, desc=\"Evaluating\"):\n",
    "            inputs = sample[\"image\"]\n",
    "            target = sample[\"target\"]\n",
    "            for layer in self.layers:\n",
    "                inputs = layer.forward(inputs)\n",
    "            forward_kl = -np.sum(target * np.log(inputs + EPSILON))\n",
    "            mse_penalty = np.sum((inputs - target) ** 2)\n",
    "            loss = forward_kl + alpha * mse_penalty\n",
    "            loss_list.append(loss)\n",
    "        \n",
    "        return np.mean(loss_list)\n",
    "\n",
    "    def infer(self, test_gen, test_count, num_samples=5, alpha=0.5):\n",
    "        self.set_mode(\"test\")\n",
    "        random_indices = set(np.random.choice(test_count, size=min(num_samples, test_count), replace=False))\n",
    "        loss_list = []\n",
    "        show_samples = []\n",
    "        \n",
    "        for index, sample in enumerate(tqdm(test_gen(), total=test_count, desc=\"Testing on novel images:\")):\n",
    "            inputs = sample[\"image\"]\n",
    "            target = sample[\"target\"]\n",
    "            for layer in self.layers:\n",
    "                inputs = layer.forward(inputs)\n",
    "\n",
    "            forward_kl = -np.sum(target * np.log(inputs + EPSILON))\n",
    "            mse_penalty = np.sum((inputs - target) ** 2)\n",
    "            loss = forward_kl + alpha * mse_penalty\n",
    "            loss_list.append(loss)\n",
    "\n",
    "            if index in random_indices:\n",
    "                show_samples.append({**sample, \"output\": inputs, \"loss\": loss})\n",
    "\n",
    "        avg_loss = np.mean(loss_list)\n",
    "        print(f\"VALIDATION: avg_loss={avg_loss}, min={np.min(loss_list)}, max={np.max(loss_list)}\")\n",
    "\n",
    "        for show_sample in show_samples:\n",
    "            flat_index = show_sample[\"output\"].argmax()\n",
    "            lat_index = flat_index // OUT_W\n",
    "            lon_index = flat_index % OUT_W \n",
    "            guess_lat = ((lat_index + 0.5) / OUT_H) * 180 - 90\n",
    "            guess_lng = ((lon_index + 0.5) / OUT_W) * 360 - 180\n",
    "            \n",
    "            actual_lat, actual_lon = show_sample[\"lat\"], show_sample[\"lon\"]\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "            axes[0].imshow(show_sample[\"target\"].reshape(OUT_H, OUT_W), cmap=\"hot\", origin=\"lower\")\n",
    "            axes[0].set_title(f\"TARGET (lat:{actual_lat}, lon:{actual_lon})\")\n",
    "            axes[0].axis(\"off\")\n",
    "            axes[1].imshow(show_sample[\"output\"].reshape(OUT_H, OUT_W), cmap=\"hot\", origin=\"lower\")\n",
    "            axes[1].set_title(f\"PREDICTION (lat:{guess_lat}, lon:{guess_lng})\")\n",
    "            axes[1].axis(\"off\")\n",
    "            axes[2].imshow(Image.open(show_sample[\"path\"]))\n",
    "            axes[2].set_title(f\"REAL IMAGE (loss: {show_sample[\"loss\"]})\")\n",
    "            axes[2].axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "                \n",
    "    def train_epoch(self, train_gen, lr=1e-3, decay=0.0, log_interval=2000, show_images=False, alpha=0.5):\n",
    "        self.set_mode(\"train\")\n",
    "        self.decay = decay\n",
    "        \n",
    "        loss_list = []\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        \n",
    "        for sample in train_gen:\n",
    "            inputs = sample[\"image\"]\n",
    "            target = sample[\"target\"]\n",
    "\n",
    "            if len(self.layers) == 0:\n",
    "                raise Exception(\"No layers to train\")\n",
    "\n",
    "            if inputs.shape[0] != len(self.layers[0]):\n",
    "                raise Exception(\"Data/input-layer shape mismatch\")\n",
    "\n",
    "            # forward pass\n",
    "            for layer in self.layers:\n",
    "                if not layer.compiled:\n",
    "                    raise Exception(\"Please compile your layers!\")\n",
    "                inputs = layer.forward(inputs)\n",
    "\n",
    "            if inputs.shape != target.shape:\n",
    "                raise Exception(\"Output-layer/target shape mismatch\")\n",
    "                \n",
    "            forward_kl = -np.sum(target * np.log(inputs + EPSILON))\n",
    "            mse_penalty = np.sum((inputs - target) ** 2)\n",
    "            \n",
    "            loss = forward_kl + alpha * mse_penalty\n",
    "            loss_list.append(loss)\n",
    "            running_loss += loss\n",
    "\n",
    "            if count > 0 and count % log_interval == 0:\n",
    "                avg_running = running_loss / log_interval\n",
    "                print(f\"[Step {count}]: avg loss: {avg_running}\")\n",
    "                running_loss = 0.0\n",
    "                \n",
    "                if show_images:\n",
    "                    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "                    axes[0].imshow(target.reshape(OUT_H, OUT_W), cmap=\"hot\", origin=\"lower\")\n",
    "                    axes[0].set_title(\"Target\")\n",
    "                    axes[0].axis(\"off\")\n",
    "                    axes[1].imshow(inputs.reshape(OUT_H, OUT_W), cmap=\"hot\", origin=\"lower\")\n",
    "                    axes[1].set_title(\"Model Output\")\n",
    "                    axes[1].axis(\"off\")\n",
    "                    axes[2].imshow(Image.open(sample[\"path\"]))\n",
    "                    axes[2].set_title(\"Original Image\")\n",
    "                    axes[2].axis(\"off\")\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "            count += 1\n",
    "            \n",
    "            # backwards pass\n",
    "            dl = -target / (inputs + EPSILON) + alpha * 2 * (inputs - target)\n",
    "            for index in range(len(self.layers) - 1, -1, -1):\n",
    "                layer = self.layers[index]\n",
    "                dl = layer.backward(dl)\n",
    "\n",
    "            # update loss on weights\n",
    "            for layer in self.layers:\n",
    "                layer.loss(lr)\n",
    "\n",
    "        return loss_list\n",
    "\n",
    "\n",
    "    def log(self):\n",
    "        total_params = 0\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, \"weights\"):\n",
    "                params = layer.weights.size + (layer.bias.size if hasattr(layer, \"bias\") else 0)\n",
    "                total_params += params\n",
    "                print(f\"{layer} ({params:,} params)\")\n",
    "            else:\n",
    "                print(layer)\n",
    "        print(f\"Total trainable parameters: {total_params:,}\")\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        state = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_state = {}\n",
    "            if hasattr(layer, \"weights\"):\n",
    "                layer_state[\"weights\"] = layer.weights\n",
    "            if hasattr(layer, \"bias\"):\n",
    "                layer_state[\"bias\"] = layer.bias\n",
    "            if hasattr(layer, \"running_mean\"):\n",
    "                layer_state[\"running_mean\"] = layer.running_mean\n",
    "                layer_state[\"running_var\"] = layer.running_var\n",
    "                layer_state[\"gamma\"] = layer.gamma\n",
    "                layer_state[\"beta\"] = layer.beta\n",
    "            if layer_state:\n",
    "                state[i] = layer_state\n",
    "        np.savez(filepath, state=state)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        data = np.load(filepath, allow_pickle=True)\n",
    "        state = data[\"state\"].item()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i in state:\n",
    "                layer_state = state[i]\n",
    "                if \"weights\" in layer_state:\n",
    "                    layer.weights = layer_state[\"weights\"]\n",
    "                if \"bias\" in layer_state:\n",
    "                    layer.bias = layer_state[\"bias\"]\n",
    "                if \"running_mean\" in layer_state:\n",
    "                    layer.running_mean = layer_state[\"running_mean\"]\n",
    "                    layer.running_var = layer_state[\"running_var\"]\n",
    "                    layer.gamma = layer_state[\"gamma\"]\n",
    "                    layer.beta = layer_state[\"beta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e58621c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Linear layer, with 3072 weights per neuron and 400 outputs (1,229,200 params)\n",
      "BatchNorm layer\n",
      "ReLU layer, with c=0.01\n",
      "Dropout layer, with p=0.2\n",
      "Dense layer, with 400 inputs and 1024 outputs (410,624 params)\n",
      "BatchNorm layer\n",
      "ReLU layer, with c=0.01\n",
      "Dropout layer, with p=0.2\n",
      "Dense layer, with 1024 inputs and 512 outputs (524,800 params)\n",
      "ReLU layer, with c=0.01\n",
      "Dense layer, with 512 inputs and 2048 outputs (1,050,624 params)\n",
      "Softmax layer\n",
      "Total trainable parameters: 3,215,248\n"
     ]
    }
   ],
   "source": [
    "model = (\n",
    "    Model(decay=1e-5)\n",
    "    .patch_linear(400, wpn=3072) # 400 patches, 3072 weights per patch\n",
    "    .batchnorm()\n",
    "    .leaky_relu(0.01)\n",
    "    .dropout(0.2)\n",
    "    .dense(1024)\n",
    "    .batchnorm()\n",
    "    .leaky_relu(0.01)\n",
    "    .dropout(0.2)\n",
    "    .dense(512)\n",
    "    .leaky_relu(0.01)\n",
    "    .dense(OUT_H * OUT_W)  # broadcast back up to heatmap coordinates\n",
    "    .softmax()\n",
    ")\n",
    "\n",
    "model.compile()\n",
    "model.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60754ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(lat, lon, height, width, sigma=1.0):\n",
    "    array = np.zeros((height, width))\n",
    "    lat_index = int((lat + 90) / 180 * height)\n",
    "    lat_index = min(height - 1, lat_index)\n",
    "    lon_index = int((lon + 180) / 360 * width)\n",
    "    lon_index = min(width - 1, lon_index)\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # gaussian distribution for heatmap\n",
    "            dist = (x - lon_index)**2 + (y - lat_index)**2\n",
    "            array[y, x] = np.exp(-dist / (2 * sigma**2))\n",
    "    array = array.flatten()\n",
    "    normalized = array / array.sum()\n",
    "    normalized = np.clip(normalized, 1e-12, None)\n",
    "    normalized = normalized / normalized.sum()\n",
    "    return normalized\n",
    "\n",
    "def load_samples(images, height, width, sigma=1.0):\n",
    "    # Count valid samples without loading images\n",
    "    count = sum(1 for img_path in images if coordinates_from_path(img_path) is not None)\n",
    "    \n",
    "    def gen():\n",
    "        for img_path in images:\n",
    "            coords = coordinates_from_path(img_path)\n",
    "            if not coords:\n",
    "                continue\n",
    "            lat, lon = coords\n",
    "            image = Image.open(img_path)\n",
    "            target = create_target(lat, lon, height, width, sigma)\n",
    "            yield Sample(\n",
    "                path=img_path,\n",
    "                lat=lat,\n",
    "                lon=lon,\n",
    "                image=format_frame(np.asarray(image)),\n",
    "                target=target\n",
    "            )\n",
    "    \n",
    "    return gen, count\n",
    "\n",
    "# Need data in the right shape\n",
    "# Incoming: [640, 640, 3] \n",
    "# Returns: -> [3 x 640 x 640]\n",
    "def format_frame(frame, patch_size=32):\n",
    "    H, W, C  = frame.shape\n",
    "    num_patches_h = H // patch_size\n",
    "    num_patches_w = W // patch_size\n",
    "    # this took SO much experimentation, but you need to do this to ensure the patches are preserved\n",
    "    frame = frame.reshape(num_patches_h, patch_size, num_patches_w, patch_size, 3)\n",
    "    frame = frame.transpose(0, 2, 1, 3, 4)\n",
    "    frame = frame.reshape(-1, patch_size, C)\n",
    "    frame = frame.transpose(2, 0, 1)\n",
    "    frame = frame.reshape(num_patches_h * num_patches_w, -1)\n",
    "    return frame / 255.0  # normalize to 0-1\n",
    "\n",
    "\n",
    "SIGMA = 2.5\n",
    "train_gen, train_count = load_samples(train, OUT_H, OUT_W, sigma=SIGMA)\n",
    "test_gen, test_count = load_samples(test, OUT_H, OUT_W, sigma=SIGMA)\n",
    "\n",
    "epochs = 12\n",
    "base_lr = 5e-4\n",
    "decay = 1e-5\n",
    "\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "print(f\"Starting training: {epochs} epochs, base_lr={base_lr}, decay={decay}\")\n",
    "print(f\"Train samples: {train_count}, Val samples: {test_count}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    lr = base_lr * (0.8 ** epoch) # exponential decay\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, rate={lr}\")\n",
    "    \n",
    "    # Train\n",
    "    loss_list = model.train_epoch(\n",
    "        tqdm(train_gen(), total=train_count, desc=f\"Training\"),\n",
    "        lr=lr, \n",
    "        decay=decay,\n",
    "        log_interval=2000,\n",
    "        show_images=True\n",
    "    )\n",
    "    train_loss = np.mean(loss_list)\n",
    "    all_train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = model.infer(test_gen, test_count, num_samples=5)\n",
    "    all_val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Train {train_loss}, val {val_loss}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model.save(f\"models/model_4_{OUT_H}x{OUT_W}_best.npz\")\n",
    "        print(f\"NEW BEST MODEL!!!!! (val_loss: {val_loss})\")\n",
    "    \n",
    "    model.save(f\"models/model_4_{OUT_H}x{OUT_W}_e{epoch + 1}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.npz\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(range(1, epochs + 1), all_train_losses, \"b-o\", label=\"Train Loss\", markersize=6)\n",
    "axes[0].plot(range(1, epochs + 1), all_val_losses, \"r-o\", label=\"Val Loss\", markersize=6)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"LOSS CURVE\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "lrs_used = [base_lr * (0.8 ** e) for e in range(epochs)]\n",
    "axes[1].plot(range(1, epochs + 1), lrs_used, \"g-o\", markersize=6)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Learning Rate\")\n",
    "axes[1].set_title(\"LEARNING RATE CURVE\")\n",
    "axes[1].set_yscale(\"log\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_graph.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Loading best model for final evaluation...\")\n",
    "model.load(f\"models/model_4_{OUT_H}x{OUT_W}_best.npz\")\n",
    "model.infer(test_gen, test_count, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f23790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_gen, train_count = load_samples(train, OUT_H, OUT_W, sigma=2)\n",
    "# model.load(\"models/model_4_32x64_e1_2025-11-21_23-50-11.npz\")\n",
    "# model.infer(train_gen, train_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
