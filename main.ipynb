{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71702eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 images for train, 4000 images for test, 107269 total!\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "import csv\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "class Sample(TypedDict):\n",
    "    path: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "    image: np.ndarray\n",
    "    target: np.ndarray\n",
    "\n",
    "def coordinates_from_path(filepath):\n",
    "    _, country, filename = filepath.split(\"/\")[-3:]\n",
    "    parts = re.split(r'[\\s_d]|h|\\.jpg', filename.replace(\"n\", \"-\"))\n",
    "    lat_deg, lat_dec, lon_deg, lon_dec, _, heading, _ = parts\n",
    "    lat = float(f\"{lat_deg}.{lat_dec}\")\n",
    "    lon = float(f\"{lon_deg}.{lon_dec}\")\n",
    "    heading = int(heading)\n",
    "    return (country, lat, lon, heading)\n",
    "\n",
    "def load_images(path, output_csv=\"coordinates.csv\"):\n",
    "    items = []\n",
    "    coordinates_dict = {}\n",
    "    \n",
    "    for country in next(os.walk(path))[1]:\n",
    "        for image in os.listdir(f\"{path}/{country}\"):\n",
    "            if \"jpg\" not in image: continue\n",
    "            filepath = f\"{path}/{country}/{image}\"\n",
    "            items.append(filepath)\n",
    "            coords = coordinates_from_path(filepath)\n",
    "            _, lat, lon, _ = coords\n",
    "            coordinates_dict[filepath] = (lat, lon)\n",
    "    \n",
    "    random.shuffle(items)\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['country', 'lat', 'lon', 'heading'])\n",
    "        for filepath in items:\n",
    "            lat, lon = coordinates_dict[filepath]\n",
    "            country = filepath.split(\"/\")[-2]\n",
    "            writer.writerow([country, lat, lon, 0])\n",
    "\n",
    "    return items\n",
    "\n",
    "def train_test_split(paths, split=0.75, max=None):\n",
    "    paths = paths[:max] if max is not None else paths\n",
    "    split_index = int(len(paths) * split)\n",
    "    train, test = paths[:split_index], paths[split_index:]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# 10,000 images, zero-indexed.png\n",
    "path = kagglehub.dataset_download(\"sylshaw/streetview-by-country\")\n",
    "# print(path)\n",
    "path = f\"{path}/streetview_images\"\n",
    "images = load_images(path)\n",
    "train, test = train_test_split(images, split=0.8)\n",
    "\n",
    "print(f\"{len(train)} images for train, {len(test)} images for test, {len(images)} total!\")\n",
    "\n",
    "# The coordinates for the output-frame (how high resolution should the probability distribution be?)\n",
    "OUT_H, OUT_W = 32, 64\n",
    "EPSILON = 1e-12 # value for ensuring model does not hit 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eef4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "# I call the inputs (incoming nodes) 'x', and outgoing (nodes that I affect) 'o'\n",
    "\n",
    "class Layer():\n",
    "    compiled = False\n",
    "    len = None\n",
    "    x = None\n",
    "    o = None\n",
    "    mode = \"train\"\n",
    "    \n",
    "    def compile(self, inputs: int = None):\n",
    "        if self.len is None:\n",
    "            if inputs is None:\n",
    "                raise Exception(\"The layer has an undefined size\")\n",
    "            self.len = inputs\n",
    "        self.compiled = True\n",
    "    \n",
    "    def loss(self, lr):\n",
    "        pass\n",
    "\n",
    "    def set_mode(self, mode: Literal[\"train\", \"test\"]):\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "# ReLU and LeakyReLU together\n",
    "class ReLU(Layer):\n",
    "    def __init__(self, c=0):\n",
    "        self.c = c\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.o = np.where(x > 0, x, self.c * x)\n",
    "        return self.o\n",
    "    \n",
    "    def backward(self, dl):\n",
    "        return dl * np.where(self.x > 0, 1, self.c)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ReLU layer, with c={self.c}\"\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.o = 1 / (1 + np.exp(-x))\n",
    "        return self.o\n",
    "    \n",
    "    def backward(self, dl):\n",
    "        return dl * self.o * (1 - self.o)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Sigmoid layer\"\n",
    "\n",
    "\n",
    "class PatchLinear(Layer):\n",
    "    def __init__(self, outputs, wpn, decay):\n",
    "        self.len = outputs\n",
    "        self.wpn = wpn\n",
    "        self.decay = decay\n",
    "\n",
    "    def compile(self, _=None):\n",
    "        if _ is not None:\n",
    "            raise Exception(\"This layer must be the first layer\")\n",
    "            \n",
    "        self.weights = np.random.randn(self.len, self.wpn) * (1 / np.sqrt(self.wpn)) # N x Inputs\n",
    "        self.bias = np.zeros(self.len)  # N x 1\n",
    "        self.compiled = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.o = []\n",
    "        for patch, weights in zip(x, self.weights):\n",
    "            self.o.append(patch.T @ weights)  # dot product, returns float\n",
    "        return np.array(self.o) + self.bias\n",
    "\n",
    "    def backward(self, dl):\n",
    "        self.dW = np.zeros_like(self.weights)\n",
    "        self.db = np.zeros_like(self.bias)\n",
    "        dx = np.zeros_like(self.x) \n",
    "\n",
    "        for i, (patch, w) in enumerate(zip(self.x, self.weights)):\n",
    "            self.dW[i] = dl[i] * patch\n",
    "            self.db[i] = dl[i]\n",
    "            dx[i] = dl[i] * w\n",
    "        return dx\n",
    "\n",
    "    def loss(self, lr):\n",
    "        self.bias -= lr * self.db\n",
    "        self.weights -= lr * (self.dW + self.decay * self.weights)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Patch Linear layer, with {self.wpn} weights per neuron and {self.len} outputs\"\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, outputs, decay):\n",
    "        self.len = outputs\n",
    "        self.decay = decay\n",
    "\n",
    "    def compile(self, inputs):\n",
    "        if inputs is None and self.inputs is None:\n",
    "            raise Exception(\"This layer has an undefined size\")\n",
    "        self.inputs = inputs\n",
    "        self.weights = np.random.randn(self.len, inputs) / np.sqrt(inputs) # N x Inputs\n",
    "        self.bias = np.zeros(self.len) # N x 1\n",
    "        self.compiled = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.o = self.weights @ x + self.bias\n",
    "        return self.o\n",
    "            \n",
    "    def backward(self, dl):\n",
    "        self.dW = np.outer(dl, self.x) # (N) x (Inputs)\n",
    "        self.db = dl # x 1\n",
    "        return self.weights.T @ dl\n",
    "    \n",
    "    def loss(self, lr):\n",
    "        self.bias -= lr * self.db\n",
    "        self.weights -= lr * (self.dW + self.decay * self.weights)\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Dense layer, with {self.inputs} inputs and {self.len} outputs\"\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        x_shift = x - np.max(x)\n",
    "        exp = np.exp(x_shift)\n",
    "        self.o = exp / np.sum(exp)\n",
    "        return self.o\n",
    "\n",
    "    def backward(self, dl):\n",
    "        return self.o * (dl - np.dot(dl, self.o))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Softmax layer\"\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == \"test\":\n",
    "            return x\n",
    "        self.mask = np.random.binomial(n=1, p=1-self.p, size=x.shape)\n",
    "        self.mask = self.mask * (1 / (1 - self.p))\n",
    "        self.o = self.mask\n",
    "        return self.o\n",
    "\n",
    "    def backward(self, dl):\n",
    "        return dl * self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Dropout layer, with p={self.p}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71443195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, decay=1e-5):\n",
    "        self.layers: List[Layer] = []\n",
    "        self.decay = decay\n",
    "\n",
    "    def sigmoid(self):\n",
    "        self.layers.append(Sigmoid())\n",
    "        return self\n",
    "    \n",
    "    def relu(self):\n",
    "        self.layers.append(ReLU())\n",
    "        return self\n",
    "\n",
    "    def leaky_relu(self, c: float):\n",
    "        self.layers.append(ReLU(c=c))\n",
    "        return self\n",
    "    \n",
    "    def patch_linear(self, outputs, wpn):\n",
    "        self.layers.append(PatchLinear(outputs, wpn, self.decay))\n",
    "        return self\n",
    "\n",
    "    def dense(self, outputs):\n",
    "        self.layers.append(Dense(outputs, self.decay))\n",
    "        return self\n",
    "\n",
    "    def softmax(self):\n",
    "        self.layers.append(Softmax())\n",
    "        return self\n",
    "    \n",
    "    def dropout(self, p):\n",
    "        self.layers.append(Dropout(p))\n",
    "        return self\n",
    "    \n",
    "    def compile(self):\n",
    "        inputs = None\n",
    "        for layer in self.layers:\n",
    "            layer.compile(inputs)\n",
    "            inputs = len(layer)\n",
    "\n",
    "    def set_mode(self, mode: Literal[\"train\", \"test\"]):\n",
    "        for layer in self.layers:\n",
    "            layer.set_mode(mode)\n",
    "\n",
    "    def infer(self, test_gen, test_count):\n",
    "        self.set_mode(\"test\")\n",
    "        random_indices = np.random.randint(0, test_count, size=10)\n",
    "        loss_list = []\n",
    "        l2_loss_list = []\n",
    "        show_samples = []\n",
    "        \n",
    "        for index, sample in enumerate(tqdm(test_gen(), total=test_count, desc=\"Testing on novel images:\")):\n",
    "            inputs = sample[\"image\"]\n",
    "            target = sample[\"target\"]\n",
    "            for layer in self.layers:\n",
    "                inputs = layer.forward(inputs)\n",
    "\n",
    "            diff = inputs - target\n",
    "            loss = np.mean(np.square(diff)) \n",
    "            loss_list.append(loss)\n",
    "\n",
    "            l2_loss = 0\n",
    "            for layer in self.layers:\n",
    "                if hasattr(layer, \"weights\"):\n",
    "                    l2_loss += 0.5 * self.decay * np.sum(layer.weights ** 2)\n",
    "            l2_loss_list.append(l2_loss)\n",
    "\n",
    "            if index in random_indices:\n",
    "                show_samples.append({**sample, \"output\": inputs, \"loss\": loss})\n",
    "\n",
    "        for show_sample in show_samples:\n",
    "            flat_index = show_sample[\"output\"].argmax()\n",
    "            lat_index = flat_index // OUT_W\n",
    "            lon_index = flat_index % OUT_W \n",
    "            guess_lat = ((lat_index + 0.5) / OUT_H) * 180 - 90\n",
    "            guess_lng = ((lon_index + 0.5) / OUT_W) * 360 - 180\n",
    "\n",
    "            plt.imshow(show_sample[\"target\"].reshape(OUT_H, OUT_W), cmap='hot', origin='lower')\n",
    "            print(f\"MODEL AVERAGE LOSS (NOVEL): {np.mean(loss_list)}\")\n",
    "            print(f\"TARGET CORRECT (lat: {show_sample['lat']}, lng: {show_sample['lon']})\")\n",
    "            plt.title(f\"TARGET CORRECT\")\n",
    "            plt.show()\n",
    "            print(f\"MODEL GUESS (guess lat: {guess_lat}, guess lng: {guess_lng})\")\n",
    "            plt.title(f\"MODEL GUESS (loss: {show_sample['loss']})\")\n",
    "            plt.imshow(show_sample[\"output\"].reshape(OUT_H, OUT_W), cmap='hot', origin='lower')\n",
    "            plt.show()\n",
    "            plt.title(f\"REAL IMAGE\")\n",
    "            plt.imshow(Image.open(show_sample[\"path\"]))\n",
    "            plt.show()\n",
    "\n",
    "        plt.hist(loss_list, bins=30)\n",
    "        plt.xlabel(\"Loss\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"NOVEL IMAGE LOSS HISTOGRAM\")\n",
    "        plt.show()\n",
    "        plt.hist(l2_loss_list, bins=30)\n",
    "        plt.xlabel(\"L2 Loss\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"NOVEL IMAGE L2 LOSS HISTOGRAM\")\n",
    "        plt.show()\n",
    "\n",
    "                \n",
    "    def train_epoch(self, train_gen, lr=1e-3):\n",
    "        self.set_mode(\"train\")\n",
    "        \n",
    "        loss_list = []\n",
    "        l2_loss_list = []\n",
    "        count = 0\n",
    "        for sample in train_gen:\n",
    "            inputs = sample[\"image\"]\n",
    "            target = sample[\"target\"]\n",
    "\n",
    "            if len(self.layers) == 0:\n",
    "                raise Exception(\"No layers to train\")\n",
    "\n",
    "            if inputs.shape[0] != len(self.layers[0]):\n",
    "                raise Exception(\"Data/input-layer shape mismatch\")\n",
    "\n",
    "            # forward pass\n",
    "            for layer in self.layers:\n",
    "                if not layer.compiled:\n",
    "                    raise Exception(\"Please compile your layers!\")\n",
    "                inputs = layer.forward(inputs)\n",
    "\n",
    "            if inputs.shape != target.shape:\n",
    "                raise Exception(\"Output-layer/target shape mismatch\")\n",
    "                \n",
    "            diff = inputs - target\n",
    "            loss = np.mean(np.square(diff)) # MSE \n",
    "            loss_list.append(loss)\n",
    "\n",
    "            # L2 regularization\n",
    "            l2_loss = 0\n",
    "            for layer in self.layers:\n",
    "                if hasattr(layer, \"weights\"):\n",
    "                    l2_loss += 0.5 * self.decay * np.sum(layer.weights ** 2)\n",
    "            l2_loss_list.append(l2_loss)\n",
    "\n",
    "            if count % 1000 == 0:\n",
    "                print(f\"Current model loss: {loss}\")\n",
    "                print(f\"Current model l2 loss: {l2_loss}\")\n",
    "            count += 1\n",
    "            \n",
    "            # backwards pass\n",
    "            N = inputs.shape[0]\n",
    "            dl = 2 * (inputs - target) / N\n",
    "            for index in range(len(self.layers) - 1, -1, -1):\n",
    "                layer = self.layers[index]\n",
    "                dl = layer.backward(dl)\n",
    "\n",
    "            # update loss on weights\n",
    "            for layer in self.layers:\n",
    "                layer.loss(lr)\n",
    "\n",
    "        return loss_list, l2_loss_list\n",
    "\n",
    "\n",
    "    def log(self):\n",
    "        for layer in self.layers:\n",
    "            print(layer)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        state = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if hasattr(layer, 'weights') and hasattr(layer, 'bias'):\n",
    "                state[i] = {'weights': layer.weights, 'bias': layer.bias}\n",
    "        np.savez(filepath, state=state)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        data = np.load(filepath, allow_pickle=True)\n",
    "        state = data['state'].item()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if hasattr(layer, 'weights') and hasattr(layer, 'bias'):\n",
    "                layer.weights = state[i]['weights']\n",
    "                layer.bias = state[i]['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e58621c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Linear layer, with 3072 weights per neuron and 400 outputs\n",
      "ReLU layer, with c=0.01\n",
      "Dense layer, with 400 inputs and 640 outputs\n",
      "ReLU layer, with c=0.01\n",
      "Dropout layer, with p=0.2\n",
      "Dense layer, with 640 inputs and 640 outputs\n",
      "ReLU layer, with c=0.01\n",
      "Dropout layer, with p=0.2\n",
      "Dense layer, with 640 inputs and 1200 outputs\n",
      "ReLU layer, with c=0.01\n",
      "Dropout layer, with p=0.3\n",
      "Dense layer, with 1200 inputs and 2048 outputs\n",
      "Softmax layer\n"
     ]
    }
   ],
   "source": [
    "model = (\n",
    "    Model()\n",
    "    .patch_linear(400, wpn=3072)  # 20 x 20 patches, 3072 weights per neuron\n",
    "    .leaky_relu(0.01)\n",
    "    .dense(640)\n",
    "    .leaky_relu(0.01)\n",
    "    .dropout(0.2)\n",
    "    .dense(640)\n",
    "    .leaky_relu(0.01)\n",
    "    .dropout(0.2)\n",
    "    .dense(1200)\n",
    "    .leaky_relu(0.01)\n",
    "    .dropout(0.3)\n",
    "    .dense(OUT_H * OUT_W) # broadcast back up to heatmap coordinates\n",
    "    .softmax()\n",
    ")\n",
    "\n",
    "model.compile()\n",
    "model.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60754ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6:   0%|          | 3/16000 [00:00<11:49, 22.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model loss: 2.0443429382652322e-05\n",
      "Current model l2 loss: 0.024645926194287675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6:   6%|▋         | 1005/16000 [00:47<11:27, 21.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model loss: 2.032025842810546e-05\n",
      "Current model l2 loss: 0.024640996813368128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6:  13%|█▎        | 2003/16000 [01:36<08:51, 26.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model loss: 2.025014654400129e-05\n",
      "Current model l2 loss: 0.024636068417175312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6:  16%|█▋        | 2640/16000 [02:07<09:12, 24.20it/s]"
     ]
    }
   ],
   "source": [
    "def create_target(lat, lon, height, width, sigma=1.0):\n",
    "    array = np.zeros((height, width))\n",
    "    lat_index = int((lat + 90) / 180 * height)\n",
    "    lat_index = min(height - 1, lat_index)\n",
    "    lon_index = int((lon + 180) / 360 * width)\n",
    "    lon_index = min(width - 1, lon_index)\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # gaussian distribution for heatmap\n",
    "            dist = (x - lon_index)**2 + (y - lat_index)**2\n",
    "            array[y, x] = np.exp(-dist / (2 * sigma**2))\n",
    "    array = array.flatten()\n",
    "    normalized = array / array.sum()\n",
    "    normalized = np.clip(normalized, 1e-12, None)\n",
    "    normalized = normalized / normalized.sum()\n",
    "    return normalized\n",
    "\n",
    "def load_samples(images, height, width, sigma=1.0):\n",
    "    # Count valid samples without loading images\n",
    "    count = sum(1 for img_path in images if coordinates_from_path(img_path) is not None)\n",
    "    \n",
    "    def gen():\n",
    "        for img_path in images:\n",
    "            coords = coordinates_from_path(img_path)\n",
    "            if not coords:\n",
    "                continue\n",
    "            _, lat, lon, _ = coords\n",
    "            image = Image.open(img_path)\n",
    "            target = create_target(lat, lon, height, width, sigma)\n",
    "            yield Sample(\n",
    "                path=img_path,\n",
    "                lat=lat,\n",
    "                lon=lon,\n",
    "                image=format_frame(np.asarray(image)),\n",
    "                target=target\n",
    "            )\n",
    "    \n",
    "    return gen, count\n",
    "\n",
    "# Need data in the right shape\n",
    "# Incoming: [640, 640, 3] \n",
    "# Returns: -> [3 x 640 x 640]\n",
    "def format_frame(frame, patch_size=32):\n",
    "    H, W, C  = frame.shape\n",
    "    num_patches_h = H // patch_size\n",
    "    num_patches_w = W // patch_size\n",
    "    # this took SO much experimentation, but you need to do this to ensure the patches are preserved\n",
    "    frame = frame.reshape(num_patches_h, patch_size, num_patches_w, patch_size, 3)\n",
    "    frame = frame.transpose(0, 2, 1, 3, 4)\n",
    "    frame = frame.reshape(-1, patch_size, C)\n",
    "    frame = frame.transpose(2, 0, 1)\n",
    "    frame = frame.reshape(num_patches_h * num_patches_w, -1)\n",
    "    return frame / 255.0  # normalize to 0-1\n",
    "\n",
    "\n",
    "SIGMA = 1.4\n",
    "train_gen, train_count = load_samples(train, OUT_H, OUT_W, sigma=SIGMA)\n",
    "test_gen, test_count = load_samples(test, OUT_H, OUT_W, sigma=SIGMA)\n",
    "\n",
    "epochs = 6\n",
    "lrs = [1e-2, 1e-2, 1e-3, 1e-3, 1e-4, 1e-4]\n",
    "for epoch, lr in zip(range(epochs), lrs):\n",
    "    loss_list, l2_loss_list = model.train_epoch(tqdm(train_gen(), total=train_count, desc=f\"Epoch {epoch + 1}/{epochs}\"), lr=1e-2)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}; avg loss: {np.mean(loss_list)}, avg l2 loss: {np.mean(l2_loss_list)}\")\n",
    "    plt.hist(loss_list, bins=60)\n",
    "    plt.xlabel(\"Loss\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"EPOCH {epoch + 1} TRAIN LOSS HISTOGRAM\")\n",
    "    plt.show()\n",
    "    plt.hist(loss_list, bins=60)\n",
    "    plt.xlabel(\"L2 Loss\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"EPOCH {epoch + 1} TRAIN L2 LOSS HISTOGRAM\")\n",
    "    plt.show()\n",
    "    model.infer(test_gen, test_count)\n",
    "    model.save(f'models/model_{OUT_H}x{OUT_W}_e{epoch + 1}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f23790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_gen, train_count = load_samples(train, OUT_H, OUT_W, sigma=2)\n",
    "# model.load('models/model_32x64_e1_2025-11-21_23-50-11.npz')\n",
    "# model.infer(train_gen, train_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
